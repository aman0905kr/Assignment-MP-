Timer unit: 1e-09 s

Total time: 2.60754 s
File: /Users/aman/Desktop/VS Code/DE Assignment (IC)/output/main_polar_MP.py
Function: final_aggregate at line 37

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    37                                           @Lprofile
    38                                           def final_aggregate() -> pl.DataFrame:
    39                                               """
    40                                               Parallelized over file‐level tasks using multiprocessing.Process + Queue.
    41                                               """
    42                                               # 1️⃣ build your list of file paths
    43         2      12000.0   6000.0      0.0      file_paths = [
    44                                                   f"/Users/aman/Desktop/VS Code/DE Assignment (IC)/Data_generation/data/mock_csvs/user_activity_{i}.csv"
    45         1          0.0      0.0      0.0          for i in range(1, 100)
    46                                               ]
    47                                           
    48                                               # 2️⃣ decide how many workers you want
    49         1       5000.0   5000.0      0.0      num_workers = min(cpu_count(), 8)
    50                                           
    51                                               # 3️⃣ create the task/result queues
    52         1    3470000.0    3e+06      0.1      task_q   = Queue()
    53         1     123000.0 123000.0      0.0      result_q = Queue()
    54                                           
    55                                               # 4️⃣ spawn the worker processes
    56         2      82000.0  41000.0      0.0      workers = [
    57                                                   Process(target=worker, args=(task_q, result_q))
    58         1       1000.0   1000.0      0.0          for _ in range(num_workers)
    59                                               ]
    60         9      18000.0   2000.0      0.0      for w in workers:
    61         8    8085000.0    1e+06      0.3          w.start()
    62                                           
    63                                               # 5️⃣ enqueue all your file paths, then send one None per worker as sentinel
    64       100      11000.0    110.0      0.0      for fp in file_paths:
    65        99     536000.0   5414.1      0.0          task_q.put(fp)
    66         9          0.0      0.0      0.0      for _ in workers:
    67         8      12000.0   1500.0      0.0          task_q.put(None)
    68                                           
    69                                               # 6️⃣ collect back the results
    70         1          0.0      0.0      0.0      dfs = []
    71         1          0.0      0.0      0.0      finished = 0
    72       108      40000.0    370.4      0.0      while finished < num_workers:
    73       107 2493247000.0    2e+07     95.6          res = result_q.get()
    74       107      71000.0    663.6      0.0          if res is None:
    75                                                       # a worker has exited
    76         8       2000.0    250.0      0.0              finished += 1
    77        99     147000.0   1484.8      0.0          elif isinstance(res, Exception):
    78                                                       # handle or log errors
    79                                                       print("Worker error:", res)
    80                                                   else:
    81        99     113000.0   1141.4      0.0              dfs.append(res)
    82                                           
    83                                               # 7️⃣ clean up
    84         9       5000.0    555.6      0.0      for w in workers:
    85         8   57968000.0    7e+06      2.2          w.join()
    86                                           
    87                                               # 8️⃣ same post‐processing as before
    88         1     867000.0 867000.0      0.0      all_df = pl.concat(dfs)
    89         1          0.0      0.0      0.0      final_df = (
    90         4   37765000.0    9e+06      1.4          all_df
    91         1          0.0      0.0      0.0          .group_by("user_id")
    92         1     112000.0 112000.0      0.0          .agg(pl.col("total_watch_time").sum().alias("total_watch_time"))
    93         1       1000.0   1000.0      0.0          .sort("user_id")
    94                                               )
    95         2    4844000.0    2e+06      0.2      final_df.write_parquet(
    96         1          0.0      0.0      0.0          "/Users/aman/Desktop/VS Code/DE Assignment (IC)/output/result.parquet_zstd_1",
    97         1          0.0      0.0      0.0          compression="zstd"
    98                                               )
    99         1          0.0      0.0      0.0      return final_df

  2.61 seconds - /Users/aman/Desktop/VS Code/DE Assignment (IC)/output/main_polar_MP.py:37 - final_aggregate
